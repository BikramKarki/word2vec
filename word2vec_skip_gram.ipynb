{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2642535c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import os\n",
    "import random\n",
    "import torch\n",
    "from d2l import torch as d2l\n",
    "import requests\n",
    "import hashlib\n",
    "import zipfile\n",
    "import collections\n",
    "import errno\n",
    "import numpy as np\n",
    "from six.moves import urllib\n",
    "from six.moves import xrange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "35b0183f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir= \"word2vec_data/words/words\"\n",
    "data_url= \"http://mattmahoney.net/dc/text8.zip\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5cc4d09e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'# sentences: 17005207'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def fetch_words_data(url=data_url, words_data=data_dir):\n",
    "    os.makedirs(words_data, exist_ok=True)\n",
    "    zip_path= os.path.join(words_data, \"words.zip\")\n",
    "    if not os.path.exists(zip_path):\n",
    "        urllib.request.urlretrieve(url, zip_path)\n",
    "    with zipfile.ZipFile(zip_path) as f:\n",
    "        data=f.read(f.namelist()[0])\n",
    "    return data.decode(\"ascii\").split()\n",
    "sentences=fetch_words_data()\n",
    "f'# sentences: {len(sentences)}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0aa117f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vocab:\n",
    "    \"\"\"Vocabulary for text.\"\"\"\n",
    "    def __init__(self, tokens=[], min_frequency=0, reserved_tokens=[]):\n",
    "\n",
    "        if tokens and isinstance(tokens[0], list):\n",
    "            tokens = [token for line in tokens for token in line]\n",
    "        # Count token frequencies\n",
    "        counter = collections.Counter(tokens)\n",
    "        self.token_frequency = sorted(counter.items(), key=lambda x: x[1],reverse=True)\n",
    "        # The list of unique tokens\n",
    "        self.idx_to_token = list(sorted(set(['<unk>'] + reserved_tokens + [token for token, frequency in self.token_frequency if frequency >= min_frequency])))\n",
    "        self.token_to_idx = {token: idx for idx, token in enumerate(self.idx_to_token)}\n",
    "    def __len__(self):\n",
    "        return len(self.idx_to_token)\n",
    "    def __getitem__(self, tokens):\n",
    "        if not isinstance(tokens, (list, tuple)):\n",
    "            return self.token_to_idx.get(tokens, self.unk)\n",
    "        return [self.__getitem__(token) for token in tokens]\n",
    "    def to_tokens(self, indices):\n",
    "        if hasattr(indices, '__len__') and len(indices) > 1:\n",
    "            return [self.idx_to_token[int(index)] for index in indices]\n",
    "        return self.idx_to_token[indices]\n",
    "    @property\n",
    "    def unk(self): #for uknown token\n",
    "        return self.token_to_idx['<unk>']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ef0114ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'vocab size: 47135'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab = Vocab(sentences, min_frequency=10)\n",
    "f'vocab size: {len(vocab)}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fab704d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def corpus_count(tokens):\n",
    "    if len(tokens) == 0 or isinstance(tokens[0], list):\n",
    "        tokens = [token for line in tokens for token in line]\n",
    "    return collections.Counter(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5224844d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def subsampling(sentences, vocab):\n",
    "    sentences = [[token for token in line if vocab[token] != vocab.unk] for line in sentences]\n",
    "    counter = corpus_count(sentences)\n",
    "    num_tokens = sum(counter.values())\n",
    "    def keep(token):\n",
    "        return (random.uniform(0, 1) < math.sqrt(1e-4 / counter[token] * num_tokens))\n",
    "    return ([[token for token in line if keep(token)] for line in sentences], counter)\n",
    "subsampled, counter = subsampling(sentences, vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1ff6dc92",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[], [], [], []]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus = [vocab[line] for line in subsampled]\n",
    "corpus[:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4a4d1147",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_centers_and_contexts(corpus, max_window_size):\n",
    "    centers, contexts = [], []\n",
    "    for line in corpus:\n",
    "        if len(line) < 2:\n",
    "            continue\n",
    "        centers += line\n",
    "        for i in range(len(line)): \n",
    "            window_size = max_window_size \n",
    "            indices = list(range(max(0, i - window_size),min(len(line), i + 1 + window_size)))\n",
    "            indices.remove(i)\n",
    "            contxt = [line[idx] for idx in indices]\n",
    "            contexts.append(contxt)\n",
    "    return centers, contexts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c6d0a1f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset [[0, 1, 2, 3, 4, 5], [6, 7, 8]]\n",
      "center 0 has contexts [1, 2]\n",
      "center 1 has contexts [0, 2, 3]\n",
      "center 2 has contexts [0, 1, 3, 4]\n",
      "center 3 has contexts [1, 2, 4, 5]\n",
      "center 4 has contexts [2, 3, 5]\n",
      "center 5 has contexts [3, 4]\n",
      "center 6 has contexts [7, 8]\n",
      "center 7 has contexts [6, 8]\n",
      "center 8 has contexts [6, 7]\n"
     ]
    }
   ],
   "source": [
    "small_dataset = [list(range(6)), list(range(6, 9))]\n",
    "print('dataset', small_dataset)\n",
    "for center, context in zip(*get_centers_and_contexts(small_dataset, 2)):\n",
    "    print('center', center, 'has contexts', context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4b5f0935",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'# center-context pairs: 919980'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_centers, all_contexts = get_centers_and_contexts(corpus, 5)\n",
    "f'# center-context pairs: {sum([len(contexts) for contexts in all_contexts])}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5c088ef1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Negative Sampling\n",
    "class RandomGenerator:\n",
    "    def __init__(self, sampling_weights):\n",
    "        self.population = list(range(1, len(sampling_weights) + 1))\n",
    "        self.sampling_weights = sampling_weights\n",
    "        self.candidates = []\n",
    "        self.i = 0\n",
    "    def draw(self):\n",
    "        if self.i == len(self.candidates):\n",
    "            self.candidates = random.choices(self.population, self.sampling_weights, k=10000)\n",
    "            self.i = 0\n",
    "        self.i += 1\n",
    "        return self.candidates[self.i - 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5ab78e69",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_negatives(all_contexts, vocab, counter, N):\n",
    "    sampling_weights = [ counter[vocab.to_tokens(i)]**0.75 for i in range(1, len(vocab))]\n",
    "    all_negatives, generator = [], RandomGenerator(sampling_weights)\n",
    "    for contexts in all_contexts:\n",
    "        negatives = []\n",
    "        while len(negatives) < len(contexts) * N:\n",
    "            neg = generator.draw()\n",
    "            if neg not in contexts:\n",
    "                negatives.append(neg)\n",
    "        all_negatives.append(negatives)\n",
    "    return all_negatives\n",
    "all_negatives = get_negatives(all_contexts, vocab, counter, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "53b7d304",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_batches(data):\n",
    "    max_len = max(len(c) + len(n) for _, c, n in data)\n",
    "    centers, contexts_negatives, masks, labels = [], [], [], []\n",
    "    for center, context, negative in data:\n",
    "        cur_len = len(context) + len(negative)\n",
    "        centers += [center]\n",
    "        contexts_negatives += [context + negative + [0] * (max_len - cur_len)]\n",
    "        masks += [[1] * cur_len + [0] * (max_len - cur_len)]\n",
    "        labels += [[1] * len(context) + [0] * (max_len - len(context))]\n",
    "    return (torch.tensor(centers).reshape((-1, 1)), torch.tensor(contexts_negatives), torch.tensor(masks),torch.tensor(labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "734e9fd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "centers = tensor([[1],\n",
      "        [1]])\n",
      "contexts_negatives = tensor([[2, 2, 3, 3, 3, 3],\n",
      "        [2, 2, 2, 3, 3, 0]])\n",
      "masks = tensor([[1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 0]])\n",
      "labels = tensor([[1, 1, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 0, 0, 0]])\n"
     ]
    }
   ],
   "source": [
    "x_1 = (1, [2, 2], [3, 3, 3, 3])\n",
    "x_2 = (1, [2, 2, 2], [3, 3])\n",
    "batch = generate_batches((x_1, x_2))\n",
    "names = ['centers', 'contexts_negatives', 'masks', 'labels']\n",
    "for name, data in zip(names, batch):\n",
    "    print(name, '=', data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "259a14a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def words_data_load(batch_size, max_window_size, num_noise_words):\n",
    "    num_workers = d2l.get_dataloader_workers()\n",
    "    sentences = fetch_words_data()\n",
    "    vocab = Vocab(sentences, min_frequency=10)\n",
    "    subsampled, counter = subsampling(sentences, vocab)\n",
    "    corpus = [vocab[line] for line in subsampled]\n",
    "    all_centers, all_contexts = get_centers_and_contexts(corpus, max_window_size)\n",
    "    all_negatives = get_negatives(all_contexts, vocab, counter,num_noise_words)\n",
    "    class Words(torch.utils.data.Dataset):\n",
    "        def __init__(self, centers, contexts, negatives):\n",
    "            assert len(centers) == len(contexts) == len(negatives) #assert in front\n",
    "            self.centers = centers\n",
    "            self.contexts = contexts\n",
    "            self.negatives = negatives\n",
    "        def __getitem__(self, index):\n",
    "            return (self.centers[index], self.contexts[index], self.negatives[index])\n",
    "        def __len__(self):\n",
    "            return len(self.centers)\n",
    "    dataset = Words(all_centers, all_contexts, all_negatives)\n",
    "    data_iter = torch.utils.data.DataLoader(dataset, batch_size, shuffle=True, collate_fn=generate_batches, num_workers=num_workers)\n",
    "    return data_iter, vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "714a7f57",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "from torch import nn\n",
    "from d2l import torch as d2l\n",
    "batch_size, max_window_size, num_noise_words = 422, 4, 5\n",
    "data_iter, vocab = words_data_load(batch_size, max_window_size,num_noise_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3850a138",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter embedding_weight (torch.Size([20, 4]), dtype=torch.float32)\n"
     ]
    }
   ],
   "source": [
    "embedd = nn.Embedding(num_embeddings=20, embedding_dim=4)\n",
    "print(f'Parameter embedding_weight ({embedd.weight.shape}, 'f'dtype={embedd.weight.dtype})')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d69e3ad3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.4135, -0.6137, -0.0303,  1.2562],\n",
       "         [-1.0078, -2.1746,  0.6127, -0.1101],\n",
       "         [ 0.4498,  0.0501,  0.4286, -0.5150]],\n",
       "\n",
       "        [[ 1.1400,  0.1930,  0.6193, -0.5477],\n",
       "         [ 0.3997, -0.3701, -0.5582,  0.3698],\n",
       "         [ 1.4657,  1.4002, -0.0736, -0.6114]]], grad_fn=<EmbeddingBackward0>)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.tensor([[1, 2, 3], [4, 5, 6]])\n",
    "embedd(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f8e5258c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def skip_gram(center, contexts_and_negatives, embedd_v, embedd_u):\n",
    "    v = embedd_v(center)\n",
    "    u = embedd_u(contexts_and_negatives)\n",
    "    pred = torch.bmm(v, u.permute(0, 2, 1))\n",
    "    return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "65ca4d46",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 1, 4])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "skip_gram(torch.ones((2, 1), dtype=torch.long),torch.ones((2, 4), dtype=torch.long), embedd, embedd).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e9c3062c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9352\n",
      "1.8462\n"
     ]
    }
   ],
   "source": [
    "def sigmd(x):\n",
    "    return -math.log(1 / (1 + math.exp(-x)))\n",
    "print(f'{(sigmd(1.1) + sigmd(2.2) + sigmd(-3.3) + sigmd(4.4)) / 4:.4f}')\n",
    "print(f'{(sigmd(-1.1) + sigmd(-2.2)) / 2:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "84b05f9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedd_size = 50\n",
    "net = nn.Sequential(nn.Embedding(num_embeddings=len(vocab), embedding_dim=embedd_size),nn.Embedding(num_embeddings=len(vocab), embedding_dim=embedd_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "5809ec25",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(net, data_iter, lr, num_epochs, device=d2l.try_gpu()):\n",
    "    def init_weights(m):\n",
    "        if type(m) == nn.Embedding:\n",
    "            nn.init.xavier_uniform_(m.weight)\n",
    "    net.apply(init_weights)\n",
    "    net = net.to(device)\n",
    "    optimizer = torch.optim.Adam(net.parameters(), lr=lr)\n",
    "    animator = d2l.Animator(xlabel='epoch', ylabel='loss',xlim=[1, num_epochs])\n",
    "    metric = d2l.Accumulator(2)\n",
    "    for epoch in range(num_epochs):\n",
    "        timer, num_batches = d2l.Timer(), len(data_iter)\n",
    "        for i, batch in enumerate(data_iter):\n",
    "            optimizer.zero_grad()\n",
    "            center, context_negative, mask, label = [data.to(device) for data in batch]\n",
    "            pred = skip_gram(center, context_negative, net[0], net[1])\n",
    "            l = (loss(pred.reshape(label.shape).float(), label.float(), mask) / mask.sum(axis=1) * mask.shape[1]) \n",
    "            l.sum().backward()\n",
    "            optimizer.step()\n",
    "            metric.add(l.sum(), l.numel())\n",
    "            if (i + 1) % (num_batches // 5) == 0 or i == num_batches - 1:\n",
    "                animator.add(epoch + (i + 1) / num_batches,(metric[0] / metric[1],))\n",
    "    print(f'loss {metric[0] / metric[1]:.3f}, 'f'{metric[1] / timer.stop():.1f} tokens/sec on {str(device)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f66fd38",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr, num_epochs= 0.002, 5\n",
    "train(net, data_iter,lr,num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "77a166e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_similar_tokens(known_token, k, embedd):\n",
    "    W = embedd.weight.data\n",
    "    x = W[vocab[known_token]]\n",
    "    cos = torch.mv(W, x) / torch.sqrt(torch.sum(W * W, dim=1) * torch.sum(x * x) + 1e-9)\n",
    "    topk = torch.topk(cos, k=k + 1)[1].cpu().numpy().astype('int32')\n",
    "    for i in topk[1:]: \n",
    "        print(f'cosine sim={float(cos[i]):.3f}: {vocab.to_tokens(i)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "54e214fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cosine sim=0.518: interglacial\n",
      "cosine sim=0.508: dementia\n",
      "cosine sim=0.506: junius\n"
     ]
    }
   ],
   "source": [
    "get_similar_tokens('baby', 3, net[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4ddcf11",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
